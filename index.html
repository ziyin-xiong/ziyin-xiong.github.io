<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <title>Ziyin Xiong</title>

    <meta name="author" content="Ziyin Xiong 熊梓因" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="shortcut icon" href="images/bear.png" type="image/x-icon" />
    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
	<meta name="google-site-verification" content="YQ9ea-yIh8xHb_5JtmpTnTIXSa3UmaqaI0NTXeiMm5A" />
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center; font-weight: normal; font-size: 25px;">
                  Ziyin Xiong&nbsp;&nbsp;|&nbsp;&nbsp;熊梓因
                </p>
                <p>I am a first-year MS Research student at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, Carnegie Mellon University, advised by Prof. <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>. I received my Bachelor's degree in Artificial Intelligence from <a href="https://tongclass.ac.cn/">Tong Class</a> (led by Prof. <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>) in <a href="https://yuanpei.pku.edu.cn/">Yuanpei College</a>, Peking University (2025).
                </p>
                <p>
                  At Peking University, I was advised by Prof. <a href="https://yzhu.io/">Yixin Zhu</a>. I also collaborated with Dr. <a href="https://siyuanhuang.com/">Siyuan Huang</a> and Dr. <a href="https://tengyu.ai/">Tengyu Liu</a> at <a href="https://eng.bigai.ai/">BIGAI</a>. In 2023 fall, I visited <a href="https://www.berkeley.edu/">UC Berkeley</a> and was honored to be advised by Prof. <a href="https://msc.berkeley.edu/people/tomizuka.html">Masayoshi Tomizuka</a>. My research interest and experience lie in the intersection of robot learning and robot vision.
                </p>
                <p style="text-align:center">
                  <a href="mailto:xiongziyin@stu.pku.edu.cn">Email</a> &nbsp;/&nbsp;
				  <a href="https://scholar.google.com/citations?user=7Rfh6rEAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ziyin-xiong/">Github</a> &nbsp;/&nbsp;
				  <a href="https://x.com/ziyin_x">X</a> &nbsp;/&nbsp;
				  <a href="https://www.linkedin.com/in/ziyin-xiong/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:35%">
                <a href="images/zyxiong.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/zyxiong.jpg"></a>
              </td>
            </tr>
          </tbody></table>

		
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;font-size:medium;">
                <p style="text-decoration:underline;text-decoration-color:#20B2AA;font-size:25px;">Research Interest</p>
                <p>
                  My research goal is to develop <strong>generalizable robot skill learning</strong> and equip robots with <strong>human-like</strong> abilities to reason and solve complex tasks. 
		It is a popular view that generalization can be achieved by learning at scale, which ignores resource cost. To address this challenge, I am exploring three aspects: 
		</p>
		<p><span style="color:#20B2AA;font-weight:bold;">Learning from off-domain data sources. </span>Human internet data provides both high-level insights into task completion and a broad task space, while implicitly revealing low-level robotic skills. Existing human data is sufficient to build representations embedding human knowledge of the world's structure and laws, which enable robots to infer spatial and physical dynamics. While this data lacks annotations, principles like consistency can be employed for effective self-supervision.</p>
		<p><span style="color:#20B2AA;font-weight:bold;">Building reward models with general prior. </span>RL rewards serve as a pivotal signal to the agent, guiding it towards desirable behaviors. However, manually crafted rewards are labor intensive and challenging to scale for unstructured real-world settings. Reward models learned from apprioriate human/robot prior could specify tasks and generalize across embodiments, which enables efficient trial-and-error learning.</p>
		<p><span style="color:#20B2AA;font-weight:bold;">Acquiring common sense from foundation models. </span>Foundation models could enhance various components of complex long-horizon robotic tasks, including perception, decision-making and control. They offer robots rich, unstructured prior knowledge, bringing my vision of imbuing robots with human-like cognitive abilities one step closer to reality.</p>
              </td>
            </tr>
          </tbody></table>

		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <p style="text-indent:20px;text-decoration:underline;text-decoration-color:#20B2AA;font-size:25px;">Selected Papers</p>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding: 20px; width: 25%; vertical-align: middle; text-align: center;"><img src="images/ag2x2.png" width=100%></td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ziyin-xiong.github.io/ag2x2.github.io/">
          <span class="papertitle">Ag2x2: A Robust Agent-Agnostic Visual Representation Boosts Zero-Shot Learning of Bimanual Robotic Manipulation</span>
        </a>
        <br>
		<strong>Ziyin Xiong*</strong>,
		Yinghan Chen*,
		<a href="https://xiaoyao-li.github.io/" style="color:#000000;">Puhao Li</a>,
		<a href="https://yzhu.io/" style="color:#000000;">Yixin Zhu</a>,
		<a href="https://tengyu.ai/" style="color:#000000;">Tengyu Liu</a>,
        	<a href="https://siyuanhuang.com/" style="color:#000000;">Siyuan Huang</a>
        <br>
        <em>IROS</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2507.19817">Paper</a>
	/
        <a href="https://ziyin-xiong.github.io/ag2x2.github.io/">Project</a>
        /
        <a href="https://github.com/ziyin-xiong/Ag2x2">Code</a>
        <p></p>
        <p>
        We present Ag2x2, a framework that advances the autonomous acquisition of bimanual manipulation skills through agent-agnostic and coordination-aware visual representations that jointly encode object and hand motion patterns.
        </p>
      </td>
    </tr>
	  </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr><p style="text-indent:20px;text-decoration:underline;text-decoration-color:#20B2AA;font-size:25px;">Selected Projects</p>
    </tr></tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding-left:20px;width:100%;vertical-align:middle">
        <span class="papertitle">AnyManip: Learning Generalizable Open-vocabulary Manipulation through Dense Optical Flows</span>
        <br>Beijing Institute for General Artificial Intelligence<br>In progress, 2024<br>
        <p></p>
        <p>
        Built on the idea that optical flow, which reveals the motion dynamics of the end effector and objects, can guide robots
in performing novel tasks in unfamiliar environments. Developed a two-stage model: an optical flow prediction module
leveraging diffusion models, trained on diverse datasets, and a motion prediction module integrating RGB observations,
optical flow, and proprioception. Achieved accurate flow prediction, action planning, and object manipulation.
        </p>
	<br>
      </td>
    </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding-left:20px;width:100%;vertical-align:middle">
        <span class="papertitle">Symmetry Regularization for Quadruped Locomotion</span>
        <br>University of California, Berkeley<br><a href="https://github.com/ziyin-xiong/Go1-Locomotion">Code</a><br>
        <p>
        In nature, quadruped animals achieve high-speed locomotion with stable, inertial, and energy-efficient postures. Inspired
by these principles, this work aimed to enhance running velocities while ensuring stable robot locomotion by leveraging
motion dynamics such as limb movement diagonal symmetry and time-reversal symmetry. Built on Legged Gym, my implementation contributed to a significant improvement in maximum tracking velocity, surpassing the reported state-of-the-art speed of the Go1 robot in simulation.
        </p>
	<br>
      </td>
    </tr>
    </tbody></table>
		
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding-left:20px;width:100%;vertical-align:middle">
        <span class="papertitle">Yuanpei Intelligence Campus</span>
	<br>Yuanpei College, Peking University<br><a href="https://yppf.yuanpei.pku.edu.cn/">Website</a>
        /
        <a href="https://github.com/Yuanpei-Intelligence/YPPF">Code</a><br>
        <p>
        An online college service system developed by Yuanpei College students, offering a convenient platform for activity room reservations, interest groups, courses, and library services. 
        I am responsible for migrating and integrating the standalone activity room reservation system with the college YPPF website and enhancing the system’s messaging functionality, working with Python.</p>
      </td>
    </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr><td style="padding:20px;width:100%;vertical-align:middle;font-size:medium;">
	<p style="text-decoration:underline;text-decoration-color:#20B2AA;font-size:25px;">Miscellaneous</p>
	<p>
	  I believe <strong>sound is the medium closest to the soul</strong>. Honest and meaningful communication can bring the world closer together.
	</p>
	<p>
	  I’m enthusiastic about podcasts for their efficiency and intimate way of sharing knowledge and ideas. I'm preparing my personal podcast channel, exploring topics including <strong>geopolitical history, traditional customs and popular culture</strong>. Contact me if you're also passionate about these subjects or interested in starting a podcast!
	</p>
      </td></tr>
    </tbody></table>
		
    <p style="text-align:center;font-size:small;">
        Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>'s website.
    </p>  
        </td>
      </tr>
    </table>
  </body>
</html>
